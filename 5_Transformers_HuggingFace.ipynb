{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e01bb6-6321-43c5-975b-244e430a7df1",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a32f13-e2d5-4c1a-b37c-5f7a252d6e91",
   "metadata": {},
   "source": [
    "## 0. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c0fbbb-c8ce-4d64-a376-4bb1cc5748c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment_VADER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23689</td>\n",
       "      <td>A21SYVGVNG8RAS</td>\n",
       "      <td>5</td>\n",
       "      <td>Low</td>\n",
       "      <td>Yummy snacks!</td>\n",
       "      <td>Popchips are the bomb!!  I use the parmesan garlic to scoop up cottage cheese as a healthy alternative to chips and dip.  My healthy eating program is saved.</td>\n",
       "      <td>0.9244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23690</td>\n",
       "      <td>AQJYXC0MPRQJL</td>\n",
       "      <td>5</td>\n",
       "      <td>Low</td>\n",
       "      <td>Great chip that is different from the rest</td>\n",
       "      <td>I like the puffed nature of this chip that makes it more unique in the chip market.  I ordered the Salt and Vinegar and absolutely love that flavor, hands down my favorite chip ever.  I have tried the cheddar and regular flavors as well.  The cheddar is about a 4/5 and the regular is about a 3/5 because I prefer strong flavors and obviously that would not be the case for the regular.  The Salt and Vinegar is kind of weak compared to some regular S&amp;V chips, but is quite flavorful and makes you wanting to come back for more.</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  ... Sentiment_VADER\n",
       "0  23689  ...          0.9244\n",
       "1  23690  ...          0.7269\n",
       "\n",
       "[2 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# modify the column width\n",
    "pd.set_option('display.max_colwidth', None) # Default is 50, None shows all text\n",
    "\n",
    "# look at a subset of the reviews\n",
    "df = pd.read_excel('Data/Popchip_Reviews_Sentiment.xlsx').head(30)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f631931-a23b-444b-9084-54cbd1527b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda59c32-a309-487c-9b86-c24d674bc36f",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfab7c0-e1eb-4392-a9f4-cebbf3b21b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb012afd-b844-46d8-866d-a6a10db9d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline('sentiment-analysis', \n",
    "                              model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                              device=-1 # -1 to use CPU\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477d4df8-dbaf-4594-b193-27264b9fc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'When life gives you lemons, make lemonade! ðŸ™‚'\n",
    "text2 = 'A dozen lemons will make a gallon of lemonade.'\n",
    "text3 = 'I didn\\'t like the taste of that lemonade at all.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a43bc61-8530-4075-957b-c231c352fd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.996239423751831}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f6ca7f-5ed6-4564-82dd-7f46b8617f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.7781572341918945}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97982ad9-e64b-46c1-9625-e25fba9df05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9955589771270752}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7fc28-6c45-470f-838f-b4883eaf0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea5ccc18-c062-4dcb-a43f-c33cbea82838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline('sentiment-analysis', \n",
    "                              model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                              device=-1, # -1 to use CPU\n",
    "                              truncation=True # Truncates text to make it shorter (Text we want to analyze)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff24194-ce2c-4acd-b74b-e8e936e84cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [{'label': 'POSITIVE', 'score': 0.9935213923454285}]\n",
       "1      [{'label': 'POSITIVE', 'score': 0.999605119228363}]\n",
       "2     [{'label': 'NEGATIVE', 'score': 0.6984866261482239}]\n",
       "3     [{'label': 'NEGATIVE', 'score': 0.9996308088302612}]\n",
       "4     [{'label': 'POSITIVE', 'score': 0.9991814494132996}]\n",
       "5     [{'label': 'POSITIVE', 'score': 0.9994196891784668}]\n",
       "6     [{'label': 'POSITIVE', 'score': 0.9992188215255737}]\n",
       "7     [{'label': 'POSITIVE', 'score': 0.9969040751457214}]\n",
       "8     [{'label': 'POSITIVE', 'score': 0.9894027709960938}]\n",
       "9     [{'label': 'POSITIVE', 'score': 0.9991832375526428}]\n",
       "10    [{'label': 'POSITIVE', 'score': 0.9994851350784302}]\n",
       "11    [{'label': 'NEGATIVE', 'score': 0.7255946397781372}]\n",
       "12    [{'label': 'POSITIVE', 'score': 0.9966173768043518}]\n",
       "13    [{'label': 'POSITIVE', 'score': 0.9997195601463318}]\n",
       "14    [{'label': 'POSITIVE', 'score': 0.8944363594055176}]\n",
       "15    [{'label': 'POSITIVE', 'score': 0.9989368319511414}]\n",
       "16    [{'label': 'POSITIVE', 'score': 0.9998534917831421}]\n",
       "17    [{'label': 'POSITIVE', 'score': 0.9663377404212952}]\n",
       "18    [{'label': 'NEGATIVE', 'score': 0.9420530200004578}]\n",
       "19     [{'label': 'POSITIVE', 'score': 0.999761164188385}]\n",
       "20    [{'label': 'NEGATIVE', 'score': 0.9653793573379517}]\n",
       "21    [{'label': 'POSITIVE', 'score': 0.9459463953971863}]\n",
       "22    [{'label': 'POSITIVE', 'score': 0.9981850981712341}]\n",
       "23    [{'label': 'POSITIVE', 'score': 0.9990038275718689}]\n",
       "24     [{'label': 'NEGATIVE', 'score': 0.752332866191864}]\n",
       "25    [{'label': 'POSITIVE', 'score': 0.9992215633392334}]\n",
       "26    [{'label': 'NEGATIVE', 'score': 0.9903901815414429}]\n",
       "27    [{'label': 'POSITIVE', 'score': 0.9994839429855347}]\n",
       "28    [{'label': 'POSITIVE', 'score': 0.9998736381530762}]\n",
       "29    [{'label': 'NEGATIVE', 'score': 0.9307065606117249}]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text.apply(sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a15769f-5fd6-4cca-a1e4-1f5504758bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment: round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede82e77-1147-4352-96f1-481b69044f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.5 s\n",
      "Wall time: 812 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [{'label': 'POSITIVE', 'score': 0.9935213923454285}]\n",
       "1      [{'label': 'POSITIVE', 'score': 0.999605119228363}]\n",
       "2     [{'label': 'NEGATIVE', 'score': 0.6984866261482239}]\n",
       "3     [{'label': 'NEGATIVE', 'score': 0.9996308088302612}]\n",
       "4     [{'label': 'POSITIVE', 'score': 0.9991814494132996}]\n",
       "5     [{'label': 'POSITIVE', 'score': 0.9994196891784668}]\n",
       "6     [{'label': 'POSITIVE', 'score': 0.9992188215255737}]\n",
       "7     [{'label': 'POSITIVE', 'score': 0.9969040751457214}]\n",
       "8     [{'label': 'POSITIVE', 'score': 0.9894027709960938}]\n",
       "9     [{'label': 'POSITIVE', 'score': 0.9991832375526428}]\n",
       "10    [{'label': 'POSITIVE', 'score': 0.9994851350784302}]\n",
       "11    [{'label': 'NEGATIVE', 'score': 0.7255946397781372}]\n",
       "12    [{'label': 'POSITIVE', 'score': 0.9966173768043518}]\n",
       "13    [{'label': 'POSITIVE', 'score': 0.9997195601463318}]\n",
       "14    [{'label': 'POSITIVE', 'score': 0.8944363594055176}]\n",
       "15    [{'label': 'POSITIVE', 'score': 0.9989368319511414}]\n",
       "16    [{'label': 'POSITIVE', 'score': 0.9998534917831421}]\n",
       "17    [{'label': 'POSITIVE', 'score': 0.9663377404212952}]\n",
       "18    [{'label': 'NEGATIVE', 'score': 0.9420530200004578}]\n",
       "19     [{'label': 'POSITIVE', 'score': 0.999761164188385}]\n",
       "20    [{'label': 'NEGATIVE', 'score': 0.9653793573379517}]\n",
       "21    [{'label': 'POSITIVE', 'score': 0.9459463953971863}]\n",
       "22    [{'label': 'POSITIVE', 'score': 0.9981850981712341}]\n",
       "23    [{'label': 'POSITIVE', 'score': 0.9990038275718689}]\n",
       "24     [{'label': 'NEGATIVE', 'score': 0.752332866191864}]\n",
       "25    [{'label': 'POSITIVE', 'score': 0.9992215633392334}]\n",
       "26    [{'label': 'NEGATIVE', 'score': 0.9903901815414429}]\n",
       "27    [{'label': 'POSITIVE', 'score': 0.9994839429855347}]\n",
       "28    [{'label': 'POSITIVE', 'score': 0.9998736381530762}]\n",
       "29    [{'label': 'NEGATIVE', 'score': 0.9307065606117249}]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# ^ Specific to jupyter notebook. It puts a magic function to see how long the cell took to run\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error() # removes confusing errors when running pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', \n",
    "                              model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                              device=-1, # -1 to use CPU\n",
    "                              truncation=True # Truncates text to make it shorter (Text we want to analyze)\n",
    "                             )\n",
    "\n",
    "df.Text.apply(sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3262d8d7-0217-418b-b76e-70bdacb484a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 175 ms\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for mps devices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m# ^ Specific to jupyter notebook. It puts a magic function to see how long the cell took to run\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfrom transformers import logging\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mlogging.set_verbosity_error() # removes confusing errors when running pipeline\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43msentiment_analyzer = pipeline(\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                              model=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistilbert/distilbert-base-uncased-finetuned-sst-2-english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                              device=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, # -1 to use CPU, \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m to use GPU\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                              truncation=True # Truncates text to make it shorter (Text we want to analyze)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                             )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mdf.Text.apply(sentiment_analyzer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1452\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1416\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1414\u001b[39m st = clock2()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:7\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1180\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1178\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:85\u001b[39m, in \u001b[36mTextClassificationPipeline.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m     88\u001b[39m         TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[32m     91\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\transformers\\pipelines\\base.py:996\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    991\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.device != \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device < \u001b[32m0\u001b[39m)\n\u001b[32m    994\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    995\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[38;5;66;03m# If the model can generate:\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28mself\u001b[39m.assistant_model, \u001b[38;5;28mself\u001b[39m.assistant_tokenizer = load_assistant_model(\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1004\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\transformers\\modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3697\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\nlp_transformers\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: PyTorch is not linked with support for mps devices"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ^ Specific to jupyter notebook. It puts a magic function to see how long the cell took to run\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error() # removes confusing errors when running pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', \n",
    "                              model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                              device='cuda:0', # -1 to use CPU, 'mps' to use apple GPU. Windows NVIDIA GPU use 'cuda' or 'cuda:0' or 0.\n",
    "                              truncation=True # Truncates text to make it shorter (Text we want to analyze)\n",
    "                             )\n",
    "\n",
    "df.Text.apply(sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3cd73-4160-4170-9681-6ca88c57c566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665b7a6-1730-4d3d-8bbf-f7a82eb610b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f79bcf-12fd-4dd5-9476-56b028eab6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
