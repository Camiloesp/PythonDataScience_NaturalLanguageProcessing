 Natural Languague Processing:
Using computers to text data. NLP falls under AI, which is a filed that tries to replicate what humans can do using computers.

AI:
1) Computer Vision: Seeing like a human.
    - Image recognition.
    - Object detection.
2) Machine Learning: Learning like a human.
    - Supervised learning.
    - Unsupervised learning.
3) NLP: Interpreting language like a human.
    - Sentiment analysis.
    - Text summarization.

Traditional NLP:
    - Rules-Based. Application: Sentiment Analysis.
    - Supervised Learning (Naive Bayes).  Application: Text Classification.
    - Unsupervised Learning (NMF).  Application: Topic Modeling.
Modern NLP:
    - Encoder-Only LLM (BERT).  Application: Sentiment Analysis, Named Entity Recognition (NER)
    - Encoder-Decoder LLM (BART).  Application: Zero Shot Classification, Text Summarization.
    - Decoder-Only LLM (GPT).  Application: Text Generation.
    - Embeddings (MiniLM).  Application: Document Similarity.

NLP Libraries in Python:
Text Preprocessing:
    - Pandas: Cleaning & Normalization.
    - SpaCy: Cleaning & Normalization, Linguistic Analysis.
    - Scikit-learn: Vectorization.
NLP with Machine Learning:
    - VADER: Sentiment Analysis.
    - Scikit-learn: Text Classification, Topic Modeling.
Neural Networks & Deep Learning:
    - Scikit-learn: Classification & Regression.
Hugging Face Transformers:
    - Transformers: Text Summarization, Text Generation, etc.

Others To Learn:
    - nltk.
    - genism.
    - TensorFlow.
    - PyTorch.

NLP Pipeline:
This workflow is found between the Cleaning Data and Exploring Data steps in the data science workflow.
Text preprocessing is about preparing raw text data for analysis and modeling:
1) We get Generally clean text data.
2) Cleaning & Normalization.
    - Cleaning: Remove unnecessary text.
    - Normalization: make text consistent.
    - These steps can be done using a combination of Pandas and spaCy
3) Vectorization.
    - Turn text into a matrix of numbers.
    - Each document is represented by a vector of counts or TF-IDF values
    - This can be done with scikit-learn.
4) Text data ready for EDA and modeling.


Text Preprocessing with spaCy:
Tokenization: let's you break text up into smaller units like words. "I'm Camilo" = ["i", "m", "camilo"] 
Lemmatization: reduces words to their base form. ["i", "m", "camilo"]  = ["I", "m", "camilo"]. Also "Selling" would change to "sell"
    Stemming:        am -> am, are -> ar, happy -> happi (BAD)
    Lemmatization:   am -> be, are -> be, happy -> happy (Better)
Stop words: words without any significant meaning.
     "I'm Camilo" = ["m", "camilo"] 
Parts of speech tagging (POS): lets you label nouns, verbs, etc. within text data.
TF-IDF (Term Frequency-Inverse Document Frequency): an alternative to the word count calculation in a DTM. It emphhasizes important words by reducing the impact of common words.


Machine Learning Algorithms for NLP: (Once you've preprocessed your text data)
- Naive Bayes (Supervised ML Algorithm for text classification): Technique that's commonly used for text classification.
- Non-Negative Matrix Factorization (Unsupervised ML Algorithm for Dimensionality reduction that works well for topic modeling):


Traditional NLP: (Small to medium data sets)
These common NLP tasks are often solved using tradition NLP methods, such as simple rules-based techniques or more advanced ML algorithms.
- Sentiment Analysis: Identifying the positivity or negativity of text. An overall sentiment score between -1 and +1 is given to each block of text.
    - Techniques: Rules-based
    - Library: VADER (Valance Aware Dictionary and Sentiment Reasoner)
    - Input format: raw text
- Text Classification: Classifying text as one label or another.
    - Technique: Supervised learning (i.e. Naive Bayes)
    - Library: scikit-learn
    - Input format: Vectorized Text (CV / TF-IDF)
- Topic Modeling: Finding themes within a corpus of text.
    - Techniques: Unsupervised learning (i.e. NMF)
    - Library: scikit-learn
    - Input format: Vectorized Text (CV / TF-IDF)

Modern NLP: (Small to large data sets)
    - Applications: Traditional NLP applications, Text Summarization, Text generation.
    - Techniques: Transformers-based LLMs (BERT, GPT, LLaMA, T5, BART)

Neural Network:
It is a machine learning model designed to process information in a way that's inspired by neurons in the human brain.
- Biological neurons communicate by receiving and passing along information to other neurons.
- A neural network processes data through layers of nodes (neurons)

Activation function: is a mathematical operation that determines the output of a neural network node
Weight: is a numerical value that determines the strength and direction of the connection between two neurons

Deep Learning: refers to a neural network with 3 or more hidden layers.

Deep Learning Architectures:
- Feedforward Neural Network (FNN): is the simplest deep learning architecture (No additional calculations or operations)
    
- Convolutional Neural Networks (CNNs): 
    Applications: Image-related trasks like image classification, object detection, etc.
- Recurrent Neural Networks (RNNs): 
    Applications: Sequential-tasks, such as NLP tasks, time series analysis, etc. 
- Transformers: 
    Applications: NLP, CV, ASR tasks, and much more!

- Large Language Models (LLMs): deep learning models that are pretrained on massive amounts of text data, including BERT and GPT.




















