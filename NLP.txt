Natural Languague Processing:
Using computers to text data. NLP falls under AI, which is a filed that tries to replicate what humans can do using computers.

AI:
1) Computer Vision: Seeing like a human.
    - Image recognition.
    - Object detection.
2) Machine Learning: Learning like a human.
    - Supervised learning.
    - Unsupervised learning.
3) NLP: Interpreting language like a human.
    - Sentiment analysis.
    - Text summarization.

Traditional NLP:
    - Rules-Based. Application: Sentiment Analysis.
    - Supervised Learning (Naive Bayes).  Application: Text Classification.
    - Unsupervised Learning (NMF).  Application: Topic Modeling.
Modern NLP:
    - Encoder-Only LLM (BERT).  Application: Sentiment Analysis, Named Entity Recognition (NER)
    - Encoder-Decoder LLM (BART).  Application: Zero Shot Classification, Text Summarization.
    - Decoder-Only LLM (GPT).  Application: Text Generation.
    - Embeddings (MiniLM).  Application: Document Similarity.

NLP Libraries in Python:
Text Preprocessing:
    - Pandas: Cleaning & Normalization.
    - SpaCy: Cleaning & Normalization, Linguistic Analysis.
    - Scikit-learn: Vectorization.
NLP with Machine Learning:
    - VADER: Sentiment Analysis.
    - Scikit-learn: Text Classification, Topic Modeling.
Neural Networks & Deep Learning:
    - Scikit-learn: Classification & Regression.
Hugging Face Transformers:
    - Transformers: Text Summarization, Text Generation, etc.

Others To Learn:
    - nltk.
    - genism.
    - TensorFlow.
    - PyTorch.

NLP Pipeline:
This workflow is found between the Cleaning Data and Exploring Data steps in the data science workflow.
Text preprocessing is about preparing raw text data for analysis and modeling:
1) We get Generally clean text data.
2) Cleaning & Normalization.
    - Cleaning: Remove unnecessary text.
    - Normalization: make text consistent.
    - These steps can be done using a combination of Pandas and spaCy
3) Vectorization.
    - Turn text into a matrix of numbers.
    - Each document is represented by a vector of counts or TF-IDF values
    - This can be done with scikit-learn.
4) Text data ready for EDA and modeling.


Text Preprocessing with spaCy:
Tokenization: let's you break text up into smaller units like words. "I'm Camilo" = ["i", "m", "camilo"] 
Lemmatization: reduces words to their base form. ["i", "m", "camilo"]  = ["I", "m", "camilo"]. Also "Selling" would change to "sell"
    Stemming:        am -> am, are -> ar, happy -> happi (BAD)
    Lemmatization:   am -> be, are -> be, happy -> happy (Better)
Stop words: words without any significant meaning.
     "I'm Camilo" = ["m", "camilo"] 
Parts of speech tagging (POS): lets you label nouns, verbs, etc. within text data.
TF-IDF (Term Frequency-Inverse Document Frequency): an alternative to the word count calculation in a DTM. It emphhasizes important words by reducing the impact of common words.


Machine Learning Algorithms for NLP: (Once you've preprocessed your text data)
- Naive Bayes (Supervised ML Algorithm for text classification):
- Non-Negative Matrix Factorization (Unsupervised ML Algorithm for Dimensionality reduction that works well for topic modeling):


Traditional NLP:
These common NLP tasks are often solved using tradition NLP methods, such as simple rules-based techniques or more advanced ML algorithms.
- Sentiment Analysis: Identifying the positivity or negativity of text.
    - Techniques: Rules-based
    - Library: VADER
    - Input format: raw text
- Text Classification: Classifying text as one label or another.
    - Technique: Supervised learning (i.e. Naive Bayes)
    - Library: scikit-learn
    - Input format: Vectorized Text (CV / TF-IDF)
- Topic Modeling: Finding themes within a corpus of text.
    - Techniques: Unsupervised learning (i.e. NMF)
    - Library: scikit-learn
    - Input format: Vectorized Text (CV / TF-IDF)





















