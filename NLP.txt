Natural Languague Processing:
Using computers to text data. NLP falls under AI, which is a filed that tries to replicate what humans can do using computers.

AI:
1) Computer Vision: Seeing like a human.
    - Image recognition.
    - Object detection.
2) Machine Learning: Learning like a human.
    - Supervised learning.
    - Unsupervised learning.
3) NLP: Interpreting language like a human.
    - Sentiment analysis.
    - Text summarization.

Traditional NLP:
    - Rules-Based. Application: Sentiment Analysis.
    - Supervised Learning (Naive Bayes).  Application: Text Classification.
    - Unsupervised Learning (NMF).  Application: Topic Modeling.
Modern NLP:
    - Encoder-Only LLM (BERT).  Application: Sentiment Analysis, Named Entity Recognition (NER)
    - Encoder-Decoder LLM (BART).  Application: Zero Shot Classification, Text Summarization.
    - Decoder-Only LLM (GPT).  Application: Text Generation.
    - Embeddings (MiniLM).  Application: Document Similarity.

NLP Libraries in Python:
Text Preprocessing:
    - Pandas: Cleaning & Normalization.
    - SpaCy: Cleaning & Normalization, Linguistic Analysis.
    - Scikit-learn: Vectorization.
NLP with Machine Learning:
    - VADER: Sentiment Analysis.
    - Scikit-learn: Text Classification, Topic Modeling.
Neural Networks & Deep Learning:
    - Scikit-learn: Classification & Regression.
Hugging Face Transformers:
    - Transformers: Text Summarization, Text Generation, etc.

Others To Learn:
    - nltk.
    - genism.
    - TensorFlow.
    - PyTorch.

NLP Pipeline:
This workflow is found between the Cleaning Data and Exploring Data steps in the data science workflow.
Text preprocessing is about preparing raw text data for analysis and modeling:
1) We get Generally clean text data.
2) Cleaning & Normalization.
    - Cleaning: Remove unnecessary text.
    - Normalization: make text consistent.
    - These steps can be done using a combination of Pandas and spaCy
3) Vectorization.
    - Turn text into a matrix of numbers.
    - Each document is represented by a vector of counts or TF-IDF values
    - This can be done with scikit-learn.
4) Text data ready for EDA and modeling.


Text Preprocessing with spaCy:
Tokenization: let's you break text up into smaller units like words. "I'm Camilo" = ["i", "m", "camilo"] 
Lemmatization: reduces words to their base form. ["i", "m", "camilo"]  = ["I", "m", "camilo"]. Also "Selling" would change to "sell"
    Stemming:        am -> am, are -> ar, happy -> happi (BAD)
    Lemmatization:   am -> be, are -> be, happy -> happy (Better)
Stop words: words without any significant meaning.
     "I'm Camilo" = ["m", "camilo"] 
Parts of speech tagging (POS): lets you label nouns, verbs, etc. within text data.






